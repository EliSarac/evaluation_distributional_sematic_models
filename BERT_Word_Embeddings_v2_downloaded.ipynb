{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "1RfUN_KolV-f",
    "outputId": "10e932f4-a95e-494f-8d3e-69a1e1be935c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dell\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lJEnBJ3gHTsQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gsyrAwYvBfC"
   },
   "source": [
    "## 2.2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WafgQPLAWmo"
   },
   "source": [
    "BERT provides its own tokenizer, which we imported above. Let's see how it handles the below sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp5zXAPBVp82"
   },
   "source": [
    "Here are some examples of the tokens contained in our vocabulary. Tokens beginning with two hashes are subwords or individual characters.\n",
    "\n",
    "*For an exploration of the contents of BERT's vocabulary, see [this notebook](https://colab.research.google.com/drive/1fCKIBJ6fgWQ-f6UKs7wDTpNTL9N-Cq9X) I created and the accompanying YouTube video [here](https://youtu.be/zJW57aCBCTk).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "1z1SzuTrqx-7",
    "outputId": "15c4ef58-a385-4650-bacb-83f0e4fd9eb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5010:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "XYjcYJuXoAQx",
    "outputId": "206d9627-954b-4e2b-8d31-cb31c49d267a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]                101\n",
      "student            3,076\n",
      "[SEP]                102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"student\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<15} {:>8,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "u_jEkVKxJMc0",
    "outputId": "48869b82-0953-4557-c36c-6066582196bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E_t4cM6KLc98"
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Mq2PKplWfbFv",
    "outputId": "883fc897-22df-446b-f14c-d60f8a734f41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4Qa5KkkM2Aq"
   },
   "source": [
    "Next, let's evaluate BERT on our example text, and fetch the hidden states of the network!\n",
    "\n",
    "*Side note: `torch.no_grad` tells PyTorch not to construct the compute graph during this forward pass (since we won't be running backprop here)--this just reduces memory consumption and speeds things up a little.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nN0QTZwiMzeq"
   },
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeQNEFbUgMSf"
   },
   "source": [
    "## 3.2. Understanding the Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "eI_uxiW7eRWA",
    "outputId": "9a3e9695-7880-4396-8765-c5abce3510b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 3\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "0CcY_oRwcHlS",
    "outputId": "3be557cf-e149-426e-caa9-766c31149403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yXZjLSke3F0"
   },
   "source": [
    "Let's combine the layers to make this one whole big tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pTJV8AFFcLbL",
    "outputId": "05334078-7859-4fa5-e09d-2ee191d89566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 3, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnBv2TUNhzf4"
   },
   "source": [
    "Let's get rid of the \"batches\" dimension since we don't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "En4JZ41fh6CI",
    "outputId": "172213e4-222d-4bb2-c3d0-cd1fb6509943"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 3, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVzRfvkbe-Yp"
   },
   "source": [
    "Finally, we can switch around the \"layers\" and \"tokens\" dimensions with `permute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AtDVE58cdeYp",
    "outputId": "2bd2b3e9-341b-46bd-d3c1-c54b2cab8230"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 13, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76TdtFH8NM9q"
   },
   "source": [
    "### Word Vectors\n",
    "\n",
    "To give you some examples, let's create word vectors two ways. \n",
    "\n",
    "First, let's **concatenate** the last four layers, giving us a single word vector per token. Each vector will have length `4 x 768 = 3,072`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnWaByfelM-e"
   },
   "source": [
    "As an alternative method, let's try creating the word vectors by **summing** together the last four layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j4DKDtFwiF0S",
    "outputId": "e29f8073-61c0-451d-bdd2-398d99ddfc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 3 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.1476e+00, -2.8162e+00, -1.4409e+00, -3.0065e+00,  1.7430e+00,\n",
      "         2.7028e+00,  2.6443e-02,  3.5562e-01,  8.6052e-01, -3.6610e+00,\n",
      "        -1.1396e+00,  3.4654e+00,  1.9338e+00, -4.4836e-01,  4.2770e-02,\n",
      "        -1.4346e+00,  5.1496e-01, -4.4730e-01,  3.3603e+00,  3.1204e+00,\n",
      "        -3.1753e+00,  7.1008e-01,  1.8145e-01, -1.9136e+00,  1.2008e+00,\n",
      "         2.8501e+00, -2.6633e-01,  4.1365e+00, -1.9186e+00,  3.9945e+00,\n",
      "         1.3614e+00, -1.4272e+00,  2.6324e+00, -4.3579e-01, -5.7935e+00,\n",
      "        -1.6424e+00, -1.0100e+00,  2.4207e+00, -3.0338e+00,  2.2198e+00,\n",
      "         1.7329e+00, -2.0004e+00,  4.4413e+00, -1.9879e+00,  7.5822e-01,\n",
      "        -1.5415e+00,  3.7527e+00,  1.8958e+00, -1.9790e+00, -2.8113e+00,\n",
      "        -2.8049e+00, -1.6436e+00, -2.9681e+00,  1.6167e+00, -3.5421e-01,\n",
      "        -2.6404e+00,  2.6281e+00,  1.2789e+00, -2.0697e+00, -2.8040e+00,\n",
      "         2.0787e-01, -3.5006e-01,  7.2415e-01, -3.4181e-01, -1.1480e+00,\n",
      "         1.2479e+00,  3.7429e+00, -8.1168e-01, -5.5966e+00,  3.6568e+00,\n",
      "         6.3863e-01,  2.2032e+00,  1.6638e+00,  5.3264e-01,  3.0959e+00,\n",
      "        -8.6370e-01,  2.0616e+00,  9.6259e-01,  8.1873e-01, -1.4990e+00,\n",
      "         3.0603e+00,  6.8964e+00,  6.5901e-03,  1.2253e+00, -1.0637e+00,\n",
      "        -1.7974e+00,  6.3860e-01, -1.5908e+00, -3.6210e+00, -2.5423e-01,\n",
      "        -2.0323e+00,  1.1870e+00, -2.2102e+00, -1.5006e+00,  2.1219e+00,\n",
      "         9.1849e-01, -3.6150e+00, -3.6955e-01, -1.5437e+00, -5.5907e+00,\n",
      "        -1.9257e+00, -4.0003e+00, -3.4412e-01,  7.5416e-01, -6.0003e-01,\n",
      "         6.5649e-02, -2.4481e+00, -6.6397e-01, -7.3661e-02,  2.6142e+00,\n",
      "        -9.5346e-01, -2.2998e-01,  1.6910e+00, -3.3278e+00,  2.5670e-01,\n",
      "         3.9049e+00,  1.3593e-01,  7.5889e-01,  1.3806e-01, -1.0437e+00,\n",
      "        -3.1711e+00,  8.4154e-01,  1.4033e+00,  2.1097e+00,  1.0690e-01,\n",
      "         2.1251e-01, -1.0495e+00, -1.2546e+00, -3.3174e-01, -2.1204e+00,\n",
      "         3.3943e+00,  2.5844e+00, -8.3940e-01,  1.7754e+00, -1.7041e+00,\n",
      "         1.5470e+00,  2.1706e+00,  2.6229e-01,  6.5724e-01, -4.0788e+00,\n",
      "         1.4693e-01, -1.6893e+00, -5.5523e-01,  4.9478e-01,  6.9730e-01,\n",
      "         1.6371e-01,  2.2085e-01,  9.1142e-01, -4.5759e+00,  4.2593e-01,\n",
      "        -2.8174e+00,  7.2437e-01, -9.2932e-01,  8.1819e-01, -4.6286e-01,\n",
      "         1.6429e+00, -2.6992e+00,  9.5737e-01, -6.9741e-01,  1.7487e+00,\n",
      "         1.0983e+00,  3.1102e+00, -1.3464e+00,  1.8862e+00, -3.4037e+00,\n",
      "        -9.0072e-01, -2.0111e+00,  3.8901e+00, -7.0291e-02, -4.4976e-01,\n",
      "        -3.9384e+00, -4.0625e+00,  4.1093e+00,  4.5389e-01, -1.3656e+00,\n",
      "         1.8592e+00, -1.0177e-01, -1.1976e+00, -7.1353e-01,  7.6548e-01,\n",
      "        -2.1357e+00,  2.7464e+00, -2.2233e+00, -2.1398e+00,  1.0343e+00,\n",
      "         1.4433e+00,  2.7000e+00, -2.8451e+00,  9.0197e-01,  7.0791e-02,\n",
      "        -6.2045e-01, -1.5113e+00, -4.2879e+00, -1.2755e+00,  7.2993e-01,\n",
      "         1.2570e+00,  6.5006e-01,  3.6569e-01,  1.9393e+00, -1.3761e+00,\n",
      "         1.3846e+00, -3.9719e-01,  1.6218e+00, -3.9545e-01, -4.4840e-01,\n",
      "         2.3358e+00,  3.1799e+00, -1.0609e+00, -1.1672e+00, -6.6015e-01,\n",
      "        -2.2110e+00,  3.1175e+00,  3.7648e-03,  1.1302e+00,  2.7177e+00,\n",
      "        -9.1628e-01,  5.7868e-01,  5.3998e-01, -7.7850e-01,  2.0130e+00,\n",
      "         3.7120e+00, -2.1635e+00,  7.8125e-01,  4.2967e-01, -4.8306e+00,\n",
      "         2.1029e+00,  2.2708e+00, -2.9823e+00,  1.9162e+00,  1.7808e+00,\n",
      "         1.2644e+00, -3.0240e+00,  1.1034e+00,  9.8414e-01, -3.2662e+00,\n",
      "         1.7685e+00, -1.3393e-01,  8.3569e-02,  1.4983e+00,  1.2465e-01,\n",
      "        -2.5893e+00, -5.0231e-02,  3.4526e+00,  3.3946e+00, -3.8609e-01,\n",
      "         1.7257e+00,  3.7204e+00, -1.4355e+00,  1.4429e+00, -1.0157e+00,\n",
      "        -1.2159e+00, -5.9761e+00, -2.5908e-01, -5.2561e-02, -2.9921e+00,\n",
      "        -1.1483e+00, -4.1076e-01, -1.8684e+00,  1.8711e+00,  5.6259e-01,\n",
      "        -1.2438e+00,  1.9449e+00,  6.2597e+00, -3.7628e+00, -2.0333e+00,\n",
      "         1.3536e+00,  1.7253e+00, -2.2146e+00,  1.1369e-01, -2.0121e+00,\n",
      "        -4.5008e+00, -4.8100e-01,  4.1347e-01,  1.3827e+00, -3.4275e+00,\n",
      "         4.4568e-01,  1.3891e+00,  2.5056e+00,  3.3477e+00, -8.4196e-01,\n",
      "         6.9815e-01, -2.9646e+00, -3.2782e+00, -4.7899e-01,  4.5386e+00,\n",
      "        -3.6598e+00, -4.5238e+00, -3.6464e-01,  4.2878e-01, -1.3666e+00,\n",
      "        -9.1048e-01,  3.8712e+00,  5.6857e-01, -2.5798e+00,  4.5114e+00,\n",
      "         3.1914e-01, -9.5144e-01,  3.6664e-01, -9.6080e-01, -1.0026e+00,\n",
      "        -3.0058e+00,  2.2352e+00,  9.5585e-01, -2.6965e+00,  2.6968e+00,\n",
      "         1.1938e+00, -5.3278e-01, -5.5386e-01, -3.0451e+01, -2.7271e-01,\n",
      "        -2.5086e-01,  1.0246e+00, -3.0487e-01, -3.0473e+00,  1.6833e-01,\n",
      "        -8.0887e-01, -3.0310e+00,  4.7012e-01,  1.5081e+00, -1.4856e-01,\n",
      "         3.6340e+00,  2.6616e+00, -1.3199e+00, -3.5779e-02, -1.4074e+00,\n",
      "        -3.3695e+00, -3.0843e+00,  5.9279e+00, -1.9498e+00, -1.5251e+00,\n",
      "         2.1843e+00,  2.4519e+00, -8.5947e-01,  5.7560e-02,  1.4052e+00,\n",
      "         1.3207e+00, -5.6434e+00, -1.6798e+00, -2.2519e+00, -1.2028e+00,\n",
      "         2.2263e-01,  1.5342e+00,  1.1817e+00, -1.2111e+00,  1.4857e+00,\n",
      "        -6.4167e-01,  5.3276e+00,  1.4287e+00,  1.9152e+00, -2.6469e-01,\n",
      "        -1.5818e+00,  3.0542e+00, -3.7168e-02,  8.5967e-01,  1.9463e-01,\n",
      "         3.8020e+00, -3.9518e-01, -1.0843e+00,  1.4851e+00,  1.1917e+00,\n",
      "         2.1240e+00, -3.5060e+00,  4.3672e-02,  5.7729e-01, -1.0520e+00,\n",
      "         2.8662e+00, -3.1664e-01, -1.1249e+00, -3.3274e-01, -7.8701e-01,\n",
      "         3.0698e-01,  2.8872e+00, -3.6236e+00,  6.7166e-01, -4.1032e+00,\n",
      "        -1.8590e+00,  2.6667e+00, -1.9302e+00,  7.6892e-01,  2.7006e-01,\n",
      "         2.4273e-01, -1.1056e+01, -3.1103e+00,  1.2674e+00, -2.6532e+00,\n",
      "        -1.8636e+00, -7.2808e-01, -2.2390e+00, -5.7712e-01,  9.0010e-01,\n",
      "         2.1340e+00,  1.4787e+00, -7.6211e-01, -4.9774e-01,  4.6235e+00,\n",
      "        -4.3215e+00, -1.9013e+00,  1.9934e+00, -3.4244e+00,  1.9727e+00,\n",
      "         1.6879e+00,  2.3706e-01,  1.9225e+00,  1.1923e-01,  1.2975e+00,\n",
      "         3.9013e-02, -6.1302e-01,  3.7805e+00, -1.4868e+00,  1.9109e+00,\n",
      "        -2.3541e+00, -7.4568e-01, -1.2997e-01, -2.1115e+00, -4.1559e+00,\n",
      "         1.7868e+00, -4.2842e-01,  3.7988e+00, -1.6363e+00,  1.2697e+00,\n",
      "         1.9789e+00,  6.5037e-01, -1.0167e+00,  1.3781e+00,  4.3444e-01,\n",
      "        -1.3533e+00,  6.2711e-01,  7.9070e-01, -2.5526e+00,  1.5087e+00,\n",
      "        -2.9001e+00, -1.3120e+00, -1.3354e+00,  1.1830e-01, -3.2551e+00,\n",
      "        -2.1309e+00, -3.8707e+00,  1.3135e+00,  7.2325e-01,  6.0425e-01,\n",
      "        -4.7469e-01,  6.0850e-01, -1.2082e+00, -2.2902e+00, -1.3620e+00,\n",
      "        -1.4615e+00, -6.4117e-01,  3.6163e+00,  3.0111e+00, -2.0794e+00,\n",
      "        -1.6646e+00, -3.1771e+00,  1.8911e+00, -1.2585e+00,  3.0358e-01,\n",
      "         1.4802e+00,  1.7822e+00,  5.3451e+00,  1.7848e+00, -5.0131e+00,\n",
      "         1.2157e+00,  3.7098e+00, -1.6039e+00,  9.7576e-02, -1.7418e+00,\n",
      "        -4.2177e+00,  1.2220e+00,  5.6028e-01, -4.2716e-01, -1.1868e+00,\n",
      "         3.0040e+00, -1.0301e+00,  4.3899e+00,  1.0290e+00, -2.3391e+00,\n",
      "         1.7281e+00, -6.1610e-01, -7.9959e-01,  3.8507e+00, -2.4498e+00,\n",
      "        -1.0707e+00,  1.9455e+00, -8.5565e-01, -2.0321e+00, -3.2205e-01,\n",
      "        -2.7224e+00, -1.3005e-01,  8.4821e-01,  4.3103e-01,  4.0349e+00,\n",
      "        -4.5364e-01, -1.8137e+00,  9.3137e-01,  1.9376e+00, -2.4309e+00,\n",
      "         6.3207e-01,  1.6598e-01,  4.3701e+00, -9.1032e-01, -1.0422e+00,\n",
      "         1.3303e+00,  7.3384e-01,  1.1117e+00, -1.9159e+00,  1.2891e+00,\n",
      "         5.5637e+00,  3.5295e+00,  2.4673e+00, -3.2236e+00, -4.2284e+00,\n",
      "         1.3587e+00,  3.2628e-01,  6.6796e-01,  3.1678e+00, -8.5951e-01,\n",
      "         2.4031e+00, -1.8526e+00, -2.2221e+00,  1.3963e-01, -3.6250e-01,\n",
      "        -1.1025e+00, -2.5980e+00, -2.8708e+00,  2.2992e+00,  1.1791e+00,\n",
      "         3.0218e+00, -6.9441e-01, -1.4246e+00, -2.3704e+00, -2.9217e+00,\n",
      "         2.3560e-01, -3.3575e+00,  1.2439e+00,  2.6306e+00, -1.3915e+00,\n",
      "        -3.3012e+00, -2.0226e+00, -2.0373e+00,  1.2681e+00,  4.0406e+00,\n",
      "         2.4953e-01, -4.1743e-01,  5.4301e+00, -2.3425e+00, -2.6754e+00,\n",
      "         9.6389e-03, -1.7712e+00,  9.2278e-01,  1.0358e+00, -4.1214e+00,\n",
      "        -4.1959e+00, -7.0059e-02, -1.5477e+00,  2.8180e+00, -1.5404e+00,\n",
      "         1.4027e-02,  5.6427e+00,  4.5537e+00, -4.7172e-01, -1.4445e+00,\n",
      "        -4.0021e-01, -1.6392e+00,  2.7436e+00, -5.2685e+00,  7.4423e-01,\n",
      "        -7.8332e-01,  5.2038e-01, -6.9875e-01, -2.1194e+00,  3.9372e+00,\n",
      "        -1.5550e+00,  2.0894e+00, -1.8293e+00, -1.6305e-01, -2.7834e-01,\n",
      "         1.6136e+00,  2.2511e+00,  1.3361e+00, -1.6658e+00, -6.7781e-01,\n",
      "         9.5725e-01,  2.3027e+00, -1.5723e+00, -1.1483e+00,  1.0752e+00,\n",
      "        -1.2669e+00, -2.0102e+00,  2.8974e-01, -1.1969e+00,  1.9955e+00,\n",
      "         1.2036e+00,  4.8700e-01,  4.3598e+00,  1.2829e+00, -1.3256e+00,\n",
      "        -1.2517e+00, -2.2591e+00, -2.4996e+00, -7.1139e-01, -1.7639e-01,\n",
      "        -2.2004e+00,  2.3132e+00, -1.3371e+00,  3.2518e+00, -1.8257e+00,\n",
      "        -2.2887e+00, -2.7634e+00, -8.9575e-01, -3.3671e+00,  3.3026e+00,\n",
      "         3.9722e-01,  5.0371e-01, -4.5542e-02,  3.5767e+00,  1.4895e+00,\n",
      "         1.3926e+00,  6.0958e-01,  1.8634e+00, -1.6165e+00, -1.5883e+00,\n",
      "        -6.3174e-01,  3.2914e+00, -1.9866e+00, -1.7712e+00,  3.0122e-01,\n",
      "        -3.7748e+00,  9.7021e-01,  4.7900e-01, -2.6393e+00, -9.1225e-01,\n",
      "         2.0204e+00,  2.5983e+00,  1.9427e+00, -4.8300e-01, -2.9155e+00,\n",
      "        -2.9357e+00,  1.2270e+00,  2.3933e+00,  7.9022e-01, -1.0246e+00,\n",
      "        -4.4482e-01,  3.3990e+00,  5.3261e+00, -1.0595e+00,  3.6712e+00,\n",
      "        -1.5265e+00, -2.9143e+00,  1.3882e-01, -5.0146e-01,  1.5128e+00,\n",
      "         1.4511e+00, -1.2774e-01,  2.6157e-02, -1.0247e-02,  1.4753e-01,\n",
      "        -9.8457e-01, -3.2158e-01,  5.2393e+00, -2.1600e+00,  1.2209e+00,\n",
      "        -2.3226e+00, -9.5464e-01, -1.6429e-01, -2.7103e+00, -1.3143e+00,\n",
      "         4.6166e-01, -2.3998e+00, -3.0250e-01,  2.1891e+00,  4.7858e-02,\n",
      "        -1.0006e+00, -1.9673e-01, -1.6607e+00, -8.7804e-01, -1.9927e-01,\n",
      "        -1.1599e+00, -1.6025e+00, -2.4076e+00, -2.3697e-01, -1.9298e+00,\n",
      "        -4.3221e-01,  3.9715e+00,  1.3047e+00, -8.3903e-01, -4.8239e-01,\n",
      "         1.7661e+00, -4.3988e+00,  9.2093e-01,  7.9513e-01,  1.8287e-01,\n",
      "         2.0892e-01, -7.7683e-01,  6.6725e+00,  9.8631e-03,  3.5828e+00,\n",
      "         2.0463e+00,  1.5764e+00,  6.4946e-01,  2.6864e+00, -3.1371e-01,\n",
      "        -3.7728e-01, -3.0816e-01,  4.1264e-01,  1.2086e+00, -6.3108e-01,\n",
      "        -2.2476e+00,  2.7285e+00, -6.1499e-01,  1.2741e+00, -1.3018e+00,\n",
      "        -6.8842e-01,  9.9620e-01, -3.6257e+00,  1.4500e+00, -1.7905e+00,\n",
      "         8.5248e-01, -3.3481e+00, -7.0958e-01,  4.0203e+00,  2.3487e+00,\n",
      "        -1.0134e+00,  1.9534e+00, -7.8954e-01,  4.3010e-01,  1.9299e+00,\n",
      "         6.1491e-01,  3.7920e+00, -1.2516e+00,  1.3469e+00, -7.1613e-01,\n",
      "        -5.4487e-02,  1.5571e+00,  1.9225e+00, -6.8894e+00, -5.2030e+00,\n",
      "        -3.3618e-01,  2.3445e+00,  3.6095e+00, -2.0762e+00,  2.8114e+00,\n",
      "         2.2400e-01,  6.7237e-01, -8.9215e-01,  1.4444e+00, -6.3648e-01,\n",
      "        -3.2054e+00, -9.7431e-01, -1.1179e+00, -2.5526e+00,  2.0980e+00,\n",
      "        -9.0327e-01, -3.7015e+00, -1.1647e+00, -4.5453e-01, -1.6999e+00,\n",
      "         1.6301e+00, -2.7474e+00, -3.5209e+00,  1.9834e+00, -2.5379e+00,\n",
      "        -5.7879e-01,  3.2023e-01,  1.0419e+00,  3.0484e-01, -1.4728e+00,\n",
      "         1.1469e+00,  2.7100e-01, -1.3213e+00])\n"
     ]
    }
   ],
   "source": [
    "print(token_vecs_sum[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQaco6jRLkXn"
   },
   "source": [
    "### Sentence Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuul6iQqnXT2"
   },
   "source": [
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the second to last hiden layer of each token producing a single 768 length vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn0n2S-FWZih"
   },
   "outputs": [],
   "source": [
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MQv0FL8VWadn",
    "outputId": "9fce910b-f15c-4545-fc0e-8d75e656f237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqYcrAipfE3E"
   },
   "source": [
    "## 3.4. Confirming contextually dependent vectors\n",
    "\n",
    "To confirm that the value of these vectors are in fact contextually dependent, let's look at the different instances of the word \"bank\" in our example sentence:\n",
    "\n",
    "\"After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.\"\n",
    "\n",
    "Let's find the index of those three instances of the word \"bank\" in the example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "DNiRsEh9cmWz",
    "outputId": "7dfa1ea6-cd85-4094-a701-de0a21914298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEhBIA5RlS8-"
   },
   "source": [
    "They are at 6, 10, and 19.\n",
    "\n",
    "For this analysis, we'll use the word vectors that we created by summing the last four layers.\n",
    "\n",
    "We can try printing out their vectors to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "tBa6vRHknSkv",
    "outputId": "8d5a547a-d9dc-4f1d-fcea-ecdd84f73f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
      "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
      "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca2TCQ_G7SM3"
   },
   "source": [
    "We can see that the values differ, but let's calculate the cosine similarity between the vectors to make a more precise comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "eYXUwiG0yhBS",
    "outputId": "38d3962f-59af-4ece-d380-e140f524380d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.94\n",
      "Vector similarity for *different* meanings:  0.69\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7jroXfKspe_"
   },
   "source": [
    "This looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orjhWUJgmxo5"
   },
   "source": [
    "## 3.5. Pooling Strategy & Layer Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1CI97kNn8dD"
   },
   "source": [
    "Below are a couple additional resources for exploring this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3D5qnRNmq5_"
   },
   "source": [
    "**BERT Authors**\n",
    "\n",
    "The BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n",
    "\n",
    "(Image from [Jay Allamar](http://jalammar.github.io/illustrated-bert/)'s blog)\n",
    "\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
    "\n",
    "While concatenation of the last four layers produced the best results on this specific task, many of the other methods come in a close second and in general it is advisable to test different versions for your specific application: results may vary.\n",
    "\n",
    "This is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7_CVgejm5pr"
   },
   "source": [
    "**Han Xiao's BERT-as-service**\n",
    "\n",
    "Han Xiao created an open-source project named [bert-as-service](https://github.com/hanxiao/bert-as-service) on GitHub which is intended to create word embeddings for your text using BERT. Han experimented with different approaches to combining these embeddings, and shared some conclusions and rationale on the [FAQ page](https://github.com/hanxiao/bert-as-service#speech_balloon-faq) of the project. \n",
    "\n",
    "`bert-as-service`, by default, uses the outputs from the **second-to-last layer** of the model. \n",
    "\n",
    "I would summarize Han's perspective by the following:\n",
    "\n",
    "1. The embeddings start out in the first layer as having no contextual information (i.e., the meaning of the initial 'bank' embedding isn't specific to river bank or financial bank).\n",
    "2. As the embeddings move deeper into the network, they pick up more and more contextual information with each layer.\n",
    "3. As you approach the final layer, however, you start picking up information that is specific to BERT's pre-training tasks (the \"Masked Language Model\" (MLM) and \"Next Sentence Prediction\" (NSP)). \n",
    "    * What we want is embeddings that encode the word meaning well... \n",
    "    * BERT is motivated to do this, but it is also motivated to encode anything else that would help it determine what a missing word is (MLM), or whether the second sentence came after the first (NSP). \n",
    "4. The second-to-last layer is what Han settled on as a reasonable sweet-spot.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONLJ36JfPuqf"
   },
   "source": [
    "# 4. Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdw7cLJWMr_Y"
   },
   "source": [
    "## 4.1. Special tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jyx2kQxbnHbM"
   },
   "source": [
    "\n",
    "It should be noted that although the `[CLS]` acts as an \"aggregate representation\" for classification tasks, this is not the best choice for a high quality sentence embedding vector. [According to](https://github.com/google-research/bert/issues/164) BERT author Jacob Devlin: \"*I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations*.\"\n",
    "\n",
    "(However, the [CLS] token does become meaningful if the model has been fine-tuned, where the last hidden layer of this token is used as the \"sentence vector\" for sequence classification.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbS8_z6XMuTJ"
   },
   "source": [
    "\n",
    "## 4.2. Out of vocabulary words\n",
    "\n",
    "For **out of vocabulary words** that are composed of multiple sentence and character-level embeddings, there is a further issue of how best to recover this embedding. Averaging the embeddings is the most straightforward solution (one that is relied upon in similar embedding models with subword vocabularies like fasttext), but summation of subword embeddings and simply taking the last token embedding (remember that the vectors are context sensitive) are acceptable alternative strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BokW7CAgMxCB"
   },
   "source": [
    "\n",
    "## 4.3. Similarity metrics\n",
    "\n",
    "It is worth noting that word-level **similarity comparisons** are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This allows wonderful things like polysemy so that e.g. your representation encodes river \"bank\" and not a financial institution \"bank\",  but makes direct word-to-word similarity comparisons less valuable. However, for sentence embeddings similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs since many similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0unZ2xh4QDap"
   },
   "source": [
    "## 4.4. Implementations\n",
    "\n",
    "You can use the code in this notebook as the foundation of your own application to extract BERT features from text. However, official [tensorflow](https://github.com/google-research/bert/blob/master/extract_features.py) and well-regarded [pytorch](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/extract_features.py) implementations already exist that do this for you.  Additionally, [bert-as-a-service](https://github.com/hanxiao/bert-as-service) is an excellent tool designed specifically for running this task with high performance, and is the one I would recommend for production applications. The author has taken great care in the tool's implementation and provides excellent documentation (some of which was used to help create this tutorial) to help users understand the more nuanced details the user faces, like resource management and pooling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbZxbKRxMvM"
   },
   "source": [
    "## Cite\n",
    "Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
