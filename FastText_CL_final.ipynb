{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4973bbc5",
   "metadata": {},
   "source": [
    "**----LOAD THE PRETRAINED FastText MODEL AND BUILD THE DICTONARY FOR WORD INDEX AND VICE-VERSA---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159b6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import io\n",
    "\n",
    "embedding_index = {}\n",
    "def LoadFastText():\n",
    "    input_file = io.open('crawl-300d-2M.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    no_of_words, vector_size = map(int, input_file.readline().split())\n",
    "    word_to_vector: Dict[str, List[float]] = dict()\n",
    "    for i, line in enumerate(input_file):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        vector = list(map(float, tokens[1:]))\n",
    "        assert len(vector) == vector_size\n",
    "        embedding_index[word] = vector\n",
    "    #return word_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2461165",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadFastText()\n",
    "i=0\n",
    "word2idx={}\n",
    "idx2word={}\n",
    "for word,v in embedding_index:\n",
    "    word2idx[word]=i\n",
    "    idx2word[i]=word\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(matrix[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275b327",
   "metadata": {},
   "source": [
    "**--------Word_Vector FUNCTION TAKES A WORD AS INPUT AND RETURNS THE CORRESPONDING WORD VECTOR-------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1391789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_Vector(word):\n",
    "    return embedding_index[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd37932",
   "metadata": {},
   "source": [
    "**------ exists FUNCTION TAKES A WORD AS PARAMETER AND RETURNS IF THE WORD EXIXTS IN THE DICTIONARY OR NOT---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1595c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(word):\n",
    "    try :\n",
    "        v=embedding_index[word]\n",
    "        return 1\n",
    "    except :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from Data_Loader import MEN_Dataloader, CC_Dataloader, gender_Dataloader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd554b",
   "metadata": {},
   "source": [
    "**--------SEMANTIC RELATEDNESS------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_1, wordList2, gold_std = MEN_Dataloader()\n",
    "cos_sim=[]\n",
    "for w1, w2 in zip(wordList_1, wordList2):\n",
    "    v1 = Word_Vector(w1).reshape(1,-1)\n",
    "    v2 = Word_Vector(w2).reshape(1,-1)\n",
    "    sim = float(cosine_similarity(v1, v2))\n",
    "    cos_sim.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df=pd.DataFrame({'Gold': gold_std, 'Cos Sim': cos_sim})\n",
    "\n",
    "print(\"---- Experiment 2: MEN dataset-------\")\n",
    "print(\"Pearson Co-relation: \\n\", similarity_df.corr(method='pearson') )\n",
    "print(\"\\nSpearman Co-relation: \\n\", similarity_df.corr(method='spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dcef8",
   "metadata": {},
   "source": [
    "**------ N_Nearest_Neighbour TAKES A WORD-VECTOR AS INPUT AND RETURNS N- NEAREST NEIGHBOURS -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_Nearest_Neighbhour(v1, n):\n",
    "    nearest_neighbours={}\n",
    "    v1=v1.reshape(1,-1)\n",
    "    for word, v in embedding_index.items():\n",
    "        v2= v.reshape(1,-1)\n",
    "        dis = 1 - cosine_similarity(v1, v2)\n",
    "        if len(nearest_neighbours)<n:\n",
    "            nearest_neighbours[word]= dis\n",
    "        else:\n",
    "            max_dis=0\n",
    "            farthest=\"\"\n",
    "            for neigh_x, dis_x in nearest_neighbours.items():\n",
    "                if dis_x> max_dis: \n",
    "                    farthest=neigh_x\n",
    "                    max_dis=dis_x\n",
    "            if dis<max_dis:\n",
    "                del nearest_neighbours[farthest]\n",
    "                nearest_neighbours[word]=dis\n",
    "    neighbours=[]\n",
    "    for neigh_x, dis_x in nearest_neighbours.items():\n",
    "        neighbours.append(neigh_x)\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f654375",
   "metadata": {},
   "source": [
    "**----------CONCEPT CATEGORIZATION --------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "test_word_embeddings = np.zeros((45, 300)) #---------- change the dimension (100 or 300) based on Golve----\n",
    "word_list, gold_standard_labels = CC_Dataloader()\n",
    "index=0\n",
    "for w in word_list:\n",
    "    w=w.lower()\n",
    "    missed=0\n",
    "    if exists(w):\n",
    "        v= np.array([np.float64(x) for x in Word_Vector(w)])\n",
    "        test_word_embeddings[index] = v\n",
    "        index+=1\n",
    "    else:\n",
    "        missed+=1\n",
    "\n",
    "print(len(test_word_embeddings))   \n",
    "kmeans = KMeans(init=\"random\", n_clusters=6, n_init=20, max_iter=100)\n",
    "result = kmeans.fit(test_word_embeddings) \n",
    "clustering_labels = result.labels_  # get the cluster ID assigned to each word embedding\n",
    "print(gold_standard_labels)\n",
    "print(clustering_labels)\n",
    "contingency_matrix = metrics.cluster.contingency_matrix(gold_standard_labels, clustering_labels) \n",
    "\n",
    "\n",
    "max_each_cluster = np.amax(contingency_matrix, axis=0)  \n",
    "total_number_datapoints = np.sum(contingency_matrix)  \n",
    "\n",
    "purity = np.sum(max_each_cluster) / total_number_datapoints  \n",
    "print(\"Contingency Matrix:\\n\", contingency_matrix)  \n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49356de9",
   "metadata": {},
   "source": [
    "**---- GENDER BIASES WITH T-TEST (P-VALUES)-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f769ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "word_list= gender_Dataloader()\n",
    "male_list=[\"he\", 'him', 'himself', 'male', 'boy', 'man', 'masculine']\n",
    "female_list=[\"she\", 'her', 'herself', 'female', 'girl', 'woman', 'feminine']\n",
    "biased=0\n",
    "non_biased=0\n",
    "for word in word_list:\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    if exists(word):\n",
    "        v=np.array([np.float64(x) for x in Word_Vector(word)]).reshape(1,-1)\n",
    "        m=0\n",
    "        f=0\n",
    "        for him, her in zip(male_list, female_list):\n",
    "            if exists(him) and exists(her):\n",
    "                v1= np.array([np.float64(x) for x in Word_Vector(him)]).reshape(1,-1)\n",
    "                v2= np.array([np.float64(x) for x in Word_Vector(her)]).reshape(1,-1)\n",
    "                sim1=(cosine_similarity(v, v1)*100)[0][0]\n",
    "                sim2=(cosine_similarity(v, v2)*100)[0][0]\n",
    "                if(sim1<0):\n",
    "                    sim1*=(-1)\n",
    "                if(sim2<0):\n",
    "                    sim2*=(-1)\n",
    "                dif=sim1-sim2\n",
    "                abs_dif=dif\n",
    "                if(dif<0):\n",
    "                    abs_dif=dif*(-1)\n",
    "                if(abs_dif>0.5 and dif>0):\n",
    "                    m+=1\n",
    "                if(abs_dif>0.5 and dif<0):\n",
    "                    f+=1\n",
    "                list1.append(sim1)\n",
    "                list2.append(sim2)\n",
    "        if(abs(m-f)>1):\n",
    "            biased+=1\n",
    "        else:\n",
    "            non_biased+=1\n",
    "        print(m, f, ttest_ind(list1, list2).pvalue)\n",
    "print(biased, non_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d6ea7",
   "metadata": {},
   "source": [
    "**----- This part is just rough code-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e07b84",
   "metadata": {},
   "source": [
    "**--------------ANALOGY TESTING---------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AN_Dataloader():\n",
    "    dataset= open('./dataset/analogy_test_reduced_2.txt')\n",
    "    word_list_1=[]\n",
    "    word_list_2=[]\n",
    "    word_list_3=[]\n",
    "    word_list_4=[]\n",
    "    for line in dataset:\n",
    "        if line.split(' ')[0]==':':\n",
    "            continue\n",
    "        w1 = line.split(' ')[0]  \n",
    "        w2 = line.split(' ')[1] \n",
    "        w3 = line.split(' ')[2]  \n",
    "        w4 = line.split(' ')[3]\n",
    "        word_list_1.append(w1)\n",
    "        word_list_2.append(w2)\n",
    "        word_list_3.append(w3)\n",
    "        word_list_4.append(w4)\n",
    "    return word_list_1, word_list_2, word_list_3, word_list_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_test():\n",
    "    word_list_1, word_list_2, word_list_3, word_list_4 = AN_Dataloader()\n",
    "    total_test = len(word_list_1)\n",
    "    missed = successful = unsuccessful = 0\n",
    "    for w1, w2, w3, w4 in zip(word_list_1, word_list_2, word_list_3, word_list_4):\n",
    "        w1=w1.lower()\n",
    "        w2=w2.lower()\n",
    "        w3=w3.lower()\n",
    "        w4=w4.lower().strip()\n",
    "        if exists(w1)==1 and exists(w2)==1 and exists(w3)==1 and exists(w4)==1 :\n",
    "            v1= np.array([np.float64(x) for x in Word_Vector(w1)])\n",
    "            v2= np.array([np.float64(x) for x in Word_Vector(w2)])\n",
    "            v3= np.array([np.float64(x) for x in Word_Vector(w3)])\n",
    "            v= v2-v1+v3\n",
    "            nearest_neighbours = N_Nearest_Neighbhour(v, 5)\n",
    "            if w4 in nearest_neighbours: \n",
    "                successful += 1\n",
    "            else:\n",
    "                unsuccessful +=1\n",
    "        else:\n",
    "            missed += 1\n",
    "    return total_test, missed, successful, unsuccessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d138fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test, missed, successful, unsuccessful = analogy_test()\n",
    "accuracy1= successful/(successful+unsuccessful)\n",
    "accuracy1= successful/(successful+unsuccessful+missed)\n",
    "print(\"Accuracy1: \", accuracy1)\n",
    "print(\"Accuracy2: \", accuracy2)\n",
    "print(successful, unsuccessful, missed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
