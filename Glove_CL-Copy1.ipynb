{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4973bbc5",
   "metadata": {},
   "source": [
    "**----LOAD THE PRETRAINED GLOVE MODEL AND BUILD THE DICTONARY FOR WORD INDEX AND VICE-VERSA---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2461165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#file_path = \"F:/Study_and_Others/LCT/Trento/study_material/Computational_Linguistics/project_idea_/pre_trained_model/glove.42B.300d\"\n",
    "embedding_index = {}\n",
    "i=0\n",
    "word2idx={}\n",
    "idx2word={}\n",
    "#with open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:])   \n",
    "        #embedding_index[word] =np.array(np.float64(coefs))\n",
    "        embedding_index[word] =coefs\n",
    "        word2idx[word]=i\n",
    "        idx2word[i]=word\n",
    "        i=i+1\n",
    "\n",
    "# create embedding matrix\n",
    "# word2idx={}\n",
    "# idx2word={}\n",
    "# i=0\n",
    "#matrix=[]    #----------------------------WE ARE NOT BUILDING THE MATRIX AS IT IS TAKING HUGE TIME------------------\n",
    "# for word, v in embedding_index.items():\n",
    "#         word2idx[word]=i\n",
    "#         idx2word[i]=word\n",
    "#         i=i+1\n",
    "        #matrix.append(embedding_index[word].astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e71523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(matrix[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275b327",
   "metadata": {},
   "source": [
    "**--------Glove_Vector FUNCTION TAKES A WORD AS INPUT AND RETURNS THE CORRESPONDING WORD VECTOR-------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1391789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glove_Vector(word):\n",
    "    return embedding_index[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd37932",
   "metadata": {},
   "source": [
    "**------ exists FUNCTION TAKES A WORD AS PARAMETER AND RETURNS IF THE WORD EXIXTS IN THE DICTIONARY OR NOT---**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1595c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(word):\n",
    "    try :\n",
    "        v=embedding_index[word]\n",
    "        return 1\n",
    "    except :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b11ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Data_Loader.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from Data_Loader import MEN_Dataloader, CC_Dataloader, gender_Dataloader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd554b",
   "metadata": {},
   "source": [
    "**--------SEMANTIC RELATEDNESS------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f91d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_1, wordList2, gold_std = MEN_Dataloader()\n",
    "cos_sim=[]\n",
    "for w1, w2 in zip(wordList_1, wordList2):\n",
    "    v1 = Glove_Vector(w1).reshape(1,-1)\n",
    "    v2 = Glove_Vector(w2).reshape(1,-1)\n",
    "    sim = float(cosine_similarity(v1, v2))\n",
    "    cos_sim.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3e29c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Experiment 2: MEN dataset-------\n",
      "Pearson Co-relation: \n",
      "              Gold   Cos Sim\n",
      "Gold     1.000000  0.696712\n",
      "Cos Sim  0.696712  1.000000\n",
      "\n",
      "Spearman Co-relation: \n",
      "              Gold   Cos Sim\n",
      "Gold     1.000000  0.693187\n",
      "Cos Sim  0.693187  1.000000\n"
     ]
    }
   ],
   "source": [
    "similarity_df=pd.DataFrame({'Gold': gold_std, 'Cos Sim': cos_sim})\n",
    "\n",
    "print(\"---- Experiment 2: MEN dataset-------\")\n",
    "print(\"Pearson Co-relation: \\n\", similarity_df.corr(method='pearson') )\n",
    "print(\"\\nSpearman Co-relation: \\n\", similarity_df.corr(method='spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dcef8",
   "metadata": {},
   "source": [
    "**------ N_Nearest_Neighbour TAKES A WORD-VECTOR AS INPUT AND RETURNS N- NEAREST NEIGHBOURS -----**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a15ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_Nearest_Neighbhour(v1, n):\n",
    "    nearest_neighbours={}\n",
    "    v1=v1.reshape(1,-1)\n",
    "    for word, v in embedding_index.items():\n",
    "        v2= v.reshape(1,-1)\n",
    "        dis = 1 - cosine_similarity(v1, v2)\n",
    "        if len(nearest_neighbours)<n:\n",
    "            nearest_neighbours[word]= dis\n",
    "        else:\n",
    "            max_dis=0\n",
    "            farthest=\"\"\n",
    "            for neigh_x, dis_x in nearest_neighbours.items():\n",
    "                if dis_x> max_dis: \n",
    "                    farthest=neigh_x\n",
    "                    max_dis=dis_x\n",
    "            if dis<max_dis:\n",
    "                del nearest_neighbours[farthest]\n",
    "                nearest_neighbours[word]=dis\n",
    "    neighbours=[]\n",
    "    for neigh_x, dis_x in nearest_neighbours.items():\n",
    "        neighbours.append(neigh_x)\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5276c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AN_Dataloader():\n",
    "    dataset= open('./dataset/analogy_test_reduced_2.txt')\n",
    "    word_list_1=[]\n",
    "    word_list_2=[]\n",
    "    word_list_3=[]\n",
    "    word_list_4=[]\n",
    "    for line in dataset:\n",
    "        if line.split(' ')[0]==':':\n",
    "            continue\n",
    "        w1 = line.split(' ')[0]  \n",
    "        w2 = line.split(' ')[1] \n",
    "        w3 = line.split(' ')[2]  \n",
    "        w4 = line.split(' ')[3]\n",
    "        word_list_1.append(w1)\n",
    "        word_list_2.append(w2)\n",
    "        word_list_3.append(w3)\n",
    "        word_list_4.append(w4)\n",
    "    return word_list_1, word_list_2, word_list_3, word_list_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e07b84",
   "metadata": {},
   "source": [
    "**--------------ANALOGY TESTING---------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241d6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_test():\n",
    "    word_list_1, word_list_2, word_list_3, word_list_4 = AN_Dataloader()\n",
    "    total_test = len(word_list_1)\n",
    "    missed = successful = unsuccessful = 0\n",
    "    for w1, w2, w3, w4 in zip(word_list_1, word_list_2, word_list_3, word_list_4):\n",
    "        w1=w1.lower()\n",
    "        w2=w2.lower()\n",
    "        w3=w3.lower()\n",
    "        w4=w4.lower().strip()\n",
    "        #print(w1, w2, w3, w4, len(w4)) #------REMOVE THIS------\n",
    "        #print(exists(w1), exists(w2), exists(w3), exists(w4))  #------REMOVE THIS------\n",
    "        if exists(w1)==1 and exists(w2)==1 and exists(w3)==1 and exists(w4)==1 :\n",
    "            #v1=Glove_Vector(w1)\n",
    "            #v2=Glove_Vector(w2)\n",
    "            #v3=Glove_Vector(w3)\n",
    "            v1= np.array([np.float64(x) for x in Glove_Vector(w1)])\n",
    "            v2= np.array([np.float64(x) for x in Glove_Vector(w2)])\n",
    "            v3= np.array([np.float64(x) for x in Glove_Vector(w3)])\n",
    "            v= v2-v1+v3\n",
    "            nearest_neighbours = N_Nearest_Neighbhour(v, 5)\n",
    "            #print(nearest_neighbours) #------REMOVE THIS------\n",
    "            if w4 in nearest_neighbours: \n",
    "                successful += 1\n",
    "            else:\n",
    "                unsuccessful +=1\n",
    "        else:\n",
    "            missed += 1\n",
    "    return total_test, missed, successful, unsuccessful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f654375",
   "metadata": {},
   "source": [
    "**----------CONCEPT CATEGORIZATION --------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6191fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chicken', 'eagle', 'duck', 'swan', 'owl', 'penguin', 'peacock', 'dog', 'elephant', 'cow', 'cat', 'lion', 'pig', 'snail', 'turtle', 'cherry', 'banana', 'pear', 'pineapple', 'mushroom', 'corn', 'lettuce', 'potato', 'onion', 'bottle', 'pencil', 'pen', 'cup', 'bowl', 'scissors', 'kettle', 'knife', 'screwdriver', 'hammer', 'spoon', 'chisel', 'telephone', 'boat', 'car', 'ship', 'truck', 'rocket', 'motorcycle', 'helicopter']\n",
      "44\n",
      "['bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'groundAnimal', 'groundAnimal', 'groundAnimal', 'groundAnimal', 'groundAnimal', 'groundAnimal', 'groundAnimal', 'groundAnimal', 'fruitTree', 'fruitTree', 'fruitTree', 'fruitTree', 'vegetable', 'vegetable', 'vegetable', 'vegetable', 'vegetable', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle']\n",
      "[5 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 5 5 5 1 5 1 1 2 2 1 1 2 5 2 2 2 1 2 4\n",
      " 3 3 3 3 3 3 3]\n",
      "Contingency Matrix:\n",
      " [[6 0 0 0 0 1]\n",
      " [0 0 0 0 0 4]\n",
      " [6 0 0 0 0 2]\n",
      " [0 4 7 0 1 1]\n",
      " [0 2 0 0 0 3]\n",
      " [0 0 0 7 0 0]]\n",
      "Purity: 0.6590909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "test_word_embeddings = np.zeros((44, 100)) #---------- change the dimension (100 or 300) based on Golve----\n",
    "word_list, gold_standard_labels = CC_Dataloader()\n",
    "print(word_list)\n",
    "index=0\n",
    "for w in word_list:\n",
    "    w=w.lower()\n",
    "    missed=0\n",
    "    if exists(w):\n",
    "        v= np.array([np.float64(x) for x in Glove_Vector(w)])\n",
    "        test_word_embeddings[index] = v\n",
    "        index+=1\n",
    "    else:\n",
    "        missed+=1\n",
    "\n",
    "print(len(test_word_embeddings))   \n",
    "kmeans = KMeans(init=\"random\", n_clusters=6, n_init=10, max_iter=50)\n",
    "result = kmeans.fit(test_word_embeddings) \n",
    "clustering_labels = result.labels_  # get the cluster ID assigned to each word embedding\n",
    "print(gold_standard_labels)\n",
    "print(clustering_labels)\n",
    "contingency_matrix = metrics.cluster.contingency_matrix(gold_standard_labels, clustering_labels) \n",
    "\n",
    "\n",
    "max_each_cluster = np.amax(contingency_matrix, axis=0)  \n",
    "total_number_datapoints = np.sum(contingency_matrix)  \n",
    "\n",
    "purity = np.sum(max_each_cluster) / total_number_datapoints  \n",
    "print(\"Contingency Matrix:\\n\", contingency_matrix)  \n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49356de9",
   "metadata": {},
   "source": [
    "**---- GENDER BIASES WITH T-TEST (P-VALUES)-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f769ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5 0.7243122183691963\n",
      "5 2 0.548678320731334\n",
      "2 5 0.9602976273289018\n",
      "5 2 0.8540946924018984\n",
      "3 4 0.9085079600345947\n",
      "6 1 0.18545361096633628\n",
      "3 4 0.89304993551109\n",
      "5 1 0.5069952435814811\n",
      "0 7 0.20907939836780545\n",
      "1 4 0.6478614019450926\n",
      "5 2 0.8334469092111655\n",
      "2 4 0.6895767092787213\n",
      "0 7 0.016841947602898463\n",
      "5 2 0.753006966861041\n",
      "3 4 0.9837106148747252\n",
      "0 7 0.1213822684394614\n",
      "4 3 0.9792941104821212\n",
      "1 6 0.06259014027963461\n",
      "1 4 0.5100226138345756\n",
      "3 2 0.9274883972980265\n",
      "2 3 0.8196467430966936\n",
      "1 6 0.3551981265956724\n",
      "4 1 0.449971114410177\n",
      "4 3 0.7080879359499401\n",
      "2 5 0.5711518486978115\n",
      "2 5 0.11793318866057048\n",
      "3 3 0.974139894788099\n",
      "2 5 0.3436642442696589\n",
      "4 3 0.6643813392722151\n",
      "3 2 0.9298261518128617\n",
      "3 4 0.9688078183078943\n",
      "0 7 0.02078667239753196\n",
      "0 7 0.11879089751480836\n",
      "3 3 0.8317454855163424\n",
      "1 6 0.08843082650240279\n",
      "2 5 0.7290222222608551\n",
      "5 1 0.21068827728674538\n",
      "1 5 0.6941529973530988\n",
      "4 3 0.8051938342009759\n",
      "3 4 0.9407623179892812\n",
      "2 3 0.99898316760423\n",
      "2 5 0.5474780290388841\n",
      "5 2 0.45862308560115317\n",
      "6 1 0.258806361430862\n",
      "2 5 0.1308244889315926\n",
      "3 4 0.8919811835781033\n",
      "5 1 0.32229101899980805\n",
      "5 2 0.08917256604339462\n",
      "1 5 0.29670508669282536\n",
      "5 1 0.29177073859801644\n",
      "1 5 0.6768115159706389\n",
      "5 2 0.2942830753719179\n",
      "7 0 0.1388029679232157\n",
      "6 0 0.3604863840314785\n",
      "5 2 0.3633441842586588\n",
      "4 2 0.6119986048685457\n",
      "5 1 0.09904034134622292\n",
      "4 3 0.6978932775349904\n",
      "6 1 0.6330292472226757\n",
      "4 1 0.7824928582230987\n",
      "6 1 0.2540822115129154\n",
      "5 2 0.4184029474122267\n",
      "5 2 0.4759446980513432\n",
      "5 2 0.19804135862636368\n",
      "6 0 0.5458412776439583\n",
      "5 2 0.27383036730336124\n",
      "0 7 0.1615041830581025\n",
      "0 7 0.1237241250264878\n",
      "1 6 0.010296605166075603\n",
      "6 1 0.5651621009996943\n",
      "4 2 0.6452438388739229\n",
      "5 1 0.7820238484456183\n",
      "6 1 0.5110006457873786\n",
      "5 1 0.5815871055100161\n",
      "1 6 0.10847238137885325\n",
      "4 2 0.8282239060622207\n",
      "5 2 0.7589064937978443\n",
      "4 3 0.6304317866678476\n",
      "4 2 0.9559206568539862\n",
      "5 1 0.33392569438071895\n",
      "6 0 0.3030074296161878\n",
      "6 1 0.5186961437340081\n",
      "2 4 0.9265626614465939\n",
      "0 7 0.322066950629354\n",
      "4 3 0.7743502877851849\n",
      "5 2 0.5998397712315612\n",
      "5 1 0.52583917171623\n",
      "4 2 0.6855387931961853\n",
      "2 4 0.6720174054415917\n",
      "2 2 0.9284113568365503\n",
      "3 4 0.9503730228638796\n",
      "5 2 0.27709329968504526\n",
      "6 1 0.6543615897989659\n",
      "5 2 0.8404196198988088\n",
      "5 2 0.47219188472691487\n",
      "6 1 0.5068087910063894\n",
      "0 6 0.2308723291624425\n",
      "6 1 0.7629753542250818\n",
      "5 1 0.5588169179097573\n",
      "0 7 0.27104800121631467\n",
      "79 21\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "word_list= gender_Dataloader()\n",
    "male_list=[\"he\", 'him', 'himself', 'male', 'boy', 'man', 'masculine']\n",
    "female_list=[\"she\", 'her', 'herself', 'female', 'girl', 'woman', 'feminine']\n",
    "biased=0\n",
    "non_biased=0\n",
    "for word in word_list:\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    if exists(word):\n",
    "        v=np.array([np.float64(x) for x in Glove_Vector(word)]).reshape(1,-1)\n",
    "        m=0\n",
    "        f=0\n",
    "        for him, her in zip(male_list, female_list):\n",
    "            if exists(him) and exists(her):\n",
    "                v1= np.array([np.float64(x) for x in Glove_Vector(him)]).reshape(1,-1)\n",
    "                v2= np.array([np.float64(x) for x in Glove_Vector(her)]).reshape(1,-1)\n",
    "                sim1=(cosine_similarity(v, v1)*100)[0][0]\n",
    "                sim2=(cosine_similarity(v, v2)*100)[0][0]\n",
    "                if(sim1<0):\n",
    "                    sim1*=(-1)\n",
    "                if(sim2<0):\n",
    "                    sim2*=(-1)\n",
    "                dif=sim1-sim2\n",
    "                abs_dif=dif\n",
    "                if(dif<0):\n",
    "                    abs_dif=dif*(-1)\n",
    "                if(abs_dif>0.5 and dif>0):\n",
    "                    m+=1\n",
    "                if(abs_dif>0.5 and dif<0):\n",
    "                    f+=1\n",
    "                list1.append(sim1)\n",
    "                list2.append(sim2)\n",
    "        if(abs(m-f)>1):\n",
    "            biased+=1\n",
    "        else:\n",
    "            non_biased+=1\n",
    "        print(m, f, ttest_ind(list1, list2).pvalue)\n",
    "print(biased, non_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2801ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy1:  0.68\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7108\\4000227489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msuccessful\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuccessful\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0munsuccessful\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuccessful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsuccessful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy2' is not defined"
     ]
    }
   ],
   "source": [
    "total_test, missed, successful, unsuccessful = analogy_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd23c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy1:  0.68\n",
      "Accuracy2:  0.68\n",
      "34 16 0\n"
     ]
    }
   ],
   "source": [
    "accuracy1= successful/(successful+unsuccessful)\n",
    "accuracy2= successful/(successful+unsuccessful+missed)\n",
    "print(\"Accuracy1: \", accuracy1)\n",
    "print(\"Accuracy2: \", accuracy2)\n",
    "print(successful, unsuccessful, missed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d6ea7",
   "metadata": {},
   "source": [
    "**----- This part is just rough code-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
